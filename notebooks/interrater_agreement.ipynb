{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sectors.utils.dataset import stratified_split\n",
    "from sectors.config import INDUSTRY_DATA_DIR, HATESPEECH_DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Agreement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PATH = INDUSTRY_DATA_DIR / \"test_preprocessed.json\"\n",
    "test = pd.read_json(TEST_PATH, lines=True)\n",
    "\n",
    "labels = [col for col in test.columns if col not in [\n",
    "    \"id\", \"legal_name\", \"description\", \"short_description\", \"tags\", \"len_des\", \"tags_string\", \"len_tags\", \"prompt\"\n",
    "    ]]\n",
    "\n",
    "xcol = [\"id\", \"legal_name\", \"description\", \"tags_string\"]\n",
    "ycol = labels\n",
    "unused, evaluation = stratified_split(test, xcol, ycol, test_size=0.12)\n",
    "evaluation.to_csv(INDUSTRY_DATA_DIR / \"evaluation.csv\", index=False)\n",
    "\n",
    "true = pd.merge(evaluation[\"id\"], test[labels + [\"id\"]], on=\"id\", how=\"left\")\n",
    "true[labels] = true[labels].astype(int)\n",
    "\n",
    "def get_rater_data(path):\n",
    "    rater1 = pd.read_csv(path)\n",
    "    rater1 = rater1.drop(columns=[\"Unnamed: 1\", \"Unnamed: 2\", \"Unnamed: 3\"])\n",
    "    rater1.loc[2] = rater1.loc[2].fillna(rater1.loc[1])\n",
    "    rater1.loc[3] = rater1.loc[3].fillna(rater1.loc[2])\n",
    "    rater1 = rater1.drop([0, 1, 2, 4, 5, 6])\n",
    "    rater1.iloc[0][\"Unnamed: 0\"] = \"id\"\n",
    "    rater1.columns = rater1.iloc[0]\n",
    "    rater1 = rater1.drop(3)\n",
    "    rater1 = rater1.reset_index(drop=True)\n",
    "    rater1.rename_axis(None, axis=1, inplace=True)\n",
    "    rater1.fillna(0, inplace=True)\n",
    "    rater1 = rater1[true.columns]\n",
    "    rater1[labels] = rater1[labels].astype(int)\n",
    "    return rater1\n",
    "\n",
    "rater1 = get_rater_data(INDUSTRY_DATA_DIR / \"annotations_rater1.csv\")\n",
    "rater2 = get_rater_data(INDUSTRY_DATA_DIR / \"annotations_rater2.csv\")\n",
    "rater3 = get_rater_data(INDUSTRY_DATA_DIR / \"annotations_rater3.csv\")\n",
    "\n",
    "def apply_threshold(a, t):\n",
    "    return np.where(a > t, 1, 0)\n",
    "\n",
    "path = \"../sectors/experiments/prompt_tuning/results/huggyllama/llama-7b/best_model/results.json\"\n",
    "model_results = json.load(open(path, \"r\"))\n",
    "test_predictions = test\n",
    "test_predictions[labels] = apply_threshold(np.array(model_results[\"test_probs\"]), 0.666).astype(int)\n",
    "test_predictions = pd.merge(evaluation[\"id\"], test_predictions[labels + [\"id\"]], on=\"id\", how=\"left\")\n",
    "\n",
    "rater1 = rater1[labels].to_numpy()\n",
    "rater2 = rater2[labels].to_numpy()\n",
    "rater3 = rater3[labels].to_numpy()\n",
    "true = true[labels].to_numpy()\n",
    "test_predictions = test_predictions[labels].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lccccc}\n",
      "\\toprule\n",
      "         & Rater2   & Rater3   & Gold   &   PTEC & \\$\\textbackslash{}Delta\\_\\{\text\\{Gold\\} - \text\\{PTEC\\}\\}\\$       \\\\\n",
      "\\midrule\n",
      " Rater1  & 0.477    & 0.401    & 0.389  &  0.36  & 0.029 \\\\\n",
      " Rater2  &          & 0.444    & 0.551  &  0.422 & 0.129 \\\\\n",
      " Rater3  &          &          & 0.311  &  0.245 & 0.066 \\\\\n",
      " Gold    &          &          &        &  0.562 &       \\\\\n",
      " Average &          &          & 0.417  &  0.342 & 0.075 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "def careful_cohen_kappa(a:np.ndarray, b:np.ndarray):\n",
    "    if np.array_equal(a, b):\n",
    "        return 1\n",
    "    else:\n",
    "        return cohen_kappa_score(a, b)\n",
    "\n",
    "\n",
    "def macro_kappa(a:np.ndarray, b:np.ndarray):\n",
    "    scores = [careful_cohen_kappa(a[:, i], b[:, i]) for i in range(a.shape[1])]\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "raters = {\"Rater1\": rater1, \"Rater2\": rater2, \"Rater3\": rater3, \"Gold\": true, \"PTEC\": test_predictions}\n",
    "kappa_values = {rater: [np.nan]*len(raters) for rater in raters.keys()}\n",
    "for i, (rater_a_name, rater_a) in enumerate(raters.items()):\n",
    "    for j, (rater_b_name, rater_b) in enumerate(raters.items()):\n",
    "        if i < j:  # Fill only one side of the diagonal\n",
    "            kappa = round(macro_kappa(rater_a, rater_b), 3)\n",
    "            kappa_values[rater_a_name][j] = kappa\n",
    "        elif i == j:  # The agreement of rater with itself is always 1\n",
    "            kappa_values[rater_a_name][j] = np.nan\n",
    "\n",
    "kappa_df = pd.DataFrame(kappa_values, index=raters.keys())\n",
    "# Calculate average kappa value for all raters, excluding 'Diff Gold - PTEC' row and diagonal\n",
    "kappa_df['Average'] = kappa_df.loc[['Gold', 'PTEC'], ['Rater1', 'Rater2', 'Rater3']].mean(axis=1).round(3)\n",
    "# Calculate the average agreement of each rater with Gold and PTEC\n",
    "kappa_df.loc['$\\Delta_{\\text{Gold} - \\text{PTEC}}$'] = (kappa_df.loc['Gold'] - kappa_df.loc['PTEC']).round(3)\n",
    "kappa_df = kappa_df.replace({np.nan: ''})\n",
    "kappa_df = kappa_df.drop(kappa_df.index[0])\n",
    "kappa_df = kappa_df.drop(['PTEC'], axis=1)\n",
    "kappa_df = kappa_df.transpose()\n",
    "table = tabulate(kappa_df, headers=\"keys\", tablefmt='latex_booktabs', showindex=\"always\")\n",
    "table = table.replace('tabular}{llllrl}', 'tabular}{lccccc}')\n",
    "print(table)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sectors",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
