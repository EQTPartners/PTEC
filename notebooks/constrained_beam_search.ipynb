{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-10 16:14:54,592] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers.utils import logging\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from sectors.config import INDUSTRY_DATA_DIR\n",
    "from sectors.utils.trie import Trie\n",
    "\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Veggies, Veggies, and Veggies'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.LongTensor(tokenizer.encode(\"Name good ingredients for a vegan salad.\", return_tensors=\"pt\")).to(device)\n",
    "out = model.generate(input_ids = input_ids, max_new_tokens = 20, num_return_sequences=1)\n",
    "tokenizer.decode(out[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'olives'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sep_token_id = 117\n",
    "tokenizer.bos_token_id = 0\n",
    "\n",
    "trie = Trie(\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    sep_token_id=tokenizer.sep_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    sequences = [\n",
    "        tokenizer.encode(lab) for lab in [\n",
    "            \"olives\",\n",
    "            \"salmon\",\n",
    "            \"seeds\",\n",
    "        ]])\n",
    "trie_fn = lambda batch_id, sent: trie.get(batch_id, sent.tolist())\n",
    "\n",
    "output = model.generate(tokenizer.encode(\"Name good ingredients for a vegan salad.\", return_tensors=\"pt\"), max_length = 20, prefix_allowed_tokens_fn=trie_fn)\n",
    "tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sector Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Trie Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Health-conscious'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"SmartHealth produces fitness trackers for health conscious individuals. This company classifies into the sectors(s): \"\n",
    "\n",
    "output = model.generate(tokenizer.encode(prompt, return_tensors=\"pt\"), max_length = 300)\n",
    "tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Trie Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Healthcare IT'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_PATH = INDUSTRY_DATA_DIR / \"train_preprocessed.json\"\n",
    "train = pd.read_json(TRAIN_PATH, lines=True)\n",
    "remove = ['id', 'legal_name', 'description', 'short_description', 'tags', 'len_des', 'tags_string', 'len_tags', 'prompt']\n",
    "labels = [col for col in train.columns if col not in remove]\n",
    "\n",
    "trie = Trie(\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    sep_token_id=tokenizer.sep_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    sequences = [\n",
    "        tokenizer.encode(lab) for lab in labels\n",
    "])\n",
    "trie_fn = lambda batch_id, sent: trie.get(batch_id, sent.tolist())\n",
    "\n",
    "output = model.generate(tokenizer.encode(prompt, return_tensors=\"pt\"), max_length = 300, prefix_allowed_tokens_fn=trie_fn)\n",
    "tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openprompt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
